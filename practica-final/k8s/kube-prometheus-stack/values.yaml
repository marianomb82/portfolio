coreDns:
  enabled: true
  service:
    selector:
      k8s-app: kube-dns
kubeControllerManager:
  enabled: false
kubeEtcd:
  enabled: true
  service:
    selector:
      k8s-app: ""
      component: etcd
kubeScheduler:
  enabled: false
kubeApiServer:
  enabled: true
kubelet:
  enabled: true
kubeDns:
  enabled: false
kubeProxy:
  enabled: true
kubeStateMetrics:
  enabled: true
nodeExporter:
  enabled: true
  jobLabel: node-exporter
  serviceMonitor:
    relabelings:
      - targetLabel: job
        replacement: node-exporter
prometheus-node-exporter:
  podLabels:
    jobLabel: node-exporter
  extraArgs:
    - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
    - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$


prometheus:
  enabled: true
  service: 
    type: LoadBalancer
  ## Configuration for Prometheus service
  prometheusSpec:
    scrapeInterval: 30s
    scrapeTimeout: 10s
    enableAdminAPI: true

    externalUrl: ""

    ruleSelectorNilUsesHelmValues: true
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false

defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: true
    configReloaders: true
    general: true
    k8s: true
    kubeApiserver: true
    kubeApiserverAvailability: true
    kubeApiserverSlos: true
    kubelet: true
    kubeProxy: true
    kubePrometheusGeneral: true
    kubePrometheusNodeRecording: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: true
    kubeScheduler: false
    kubeStateMetrics: true
    network: true
    node: true
    nodeExporterAlerting: true
    nodeExporterRecording: true
    prometheus: true
    prometheusOperator: true

## Configuration for alertmanager
alertmanager:
  enabled: true
  service: 
    type: LoadBalancer 
  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ['job']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'slack'
      routes:
      - match:
          alertname: Watchdog
        receiver: 'null'
    # This inhibt rule is a hack from: https://stackoverflow.com/questions/54806336/how-to-silence-prometheus-alertmanager-using-config-files/54814033#54814033
    inhibit_rules:
      - target_match_re:
           alertname: '.+Overcommit'
        source_match:
           alertname: 'Watchdog'
        equal: ['prometheus']
    receivers:
    - name: 'null'
    - name: 'slack'
      slack_configs:
      - api_url: 'https://hooks.slack.com/services/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' # <--- AÑADIR EN ESTA LÍNEA EL WEBHOOK CREADO
        send_resolved: true
        channel: '#paradigma' # <--- AÑADIR EN ESTA LÍNEA EL CANAL
        title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] Monitoring Event Notification'
        text: |-
          {{ range .Alerts }}
            *Alert:* {{ .Labels.alertname }} - `{{ .Labels.severity }}`
            *Description:* {{ .Annotations.message }}
            *Graph:* <{{ .GeneratorURL }}|:chart_with_upwards_trend:> *Runbook:* <{{ .Annotations.runbook_url }}|:spiral_note_pad:>
            *Details:*
            {{ range .Labels.SortedPairs }} • *{{ .Name }}:* `{{ .Value }}`
            {{ end }}
          {{ end }}

## Manages Prometheus and Alertmanager components
prometheusOperator:
  enabled: true

additionalPrometheusRulesMap:
  forced-alert-rules:
    groups:
      - name: forced_ruleset_1
        rules:
          - alert: paradigmaForced
            expr: sum(container_memory_usage_bytes{namespace="paradigma"}) > 1
            for: 0s
            labels:
              severity: warning
              alertname: "forced paradigma alert"
            annotations:
              summary: Pod {{ $labels.pod }} forced alert
              description: "Forced alert"

  paradigma-alert-rules:
    groups:
      - name: paradigma_ruleset_1
        rules:
          - alert: paradigmaConsumingMoreThanRequest
            expr: sum(avg(container_memory_usage_bytes{namespace="paradigma"}) by (pod)) > sum(avg(kube_pod_container_resource_requests{resource="memory",namespace="paradigma"})) by (pod)
            for: 0s
            labels:
              severity: critical
              alertname: "paradigma consuming more memory than requested"
            annotations:
              summary: Pod {{ $labels.pod }} consuming more memory than requested
              description: "Pod more less than request"
              message: Pod {{ $labels.pod }} is consuming more memory than requested

#Así añadimos el loadbalabcer en el grafana
grafana:
  service:
    type: LoadBalancer
